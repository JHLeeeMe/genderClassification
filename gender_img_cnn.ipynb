{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = './resources/images/gender/trainSet/'\n",
    "Labels = os.listdir(TRAIN_DIR)\n",
    "csvList = []\n",
    "\n",
    "for dir in Labels:\n",
    "    img_paths = glob.glob(TRAIN_DIR + dir + '/*')\n",
    "    imgs = [cv2.imread(file, 0) for file in img_paths]\n",
    "    \n",
    "    for img in imgs:\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img_flat = img.flatten()\n",
    "        csvList.append(np.append(img_flat, int(dir.split('_')[0])))\n",
    "        \n",
    "        # image flip\n",
    "        vertical_flip_img = cv2.flip(img, 1)\n",
    "        vertical_flip_img = np.array(vertical_flip_img, dtype=np.float32)\n",
    "        vertical_flip_img_flat = vertical_flip_img.flatten()\n",
    "        csvList.append(np.append(vertical_flip_img_flat, int(dir.split('_')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists...\n"
     ]
    }
   ],
   "source": [
    "csv_path = './resources/csv/gender/train_gender_img_label_data.csv'\n",
    "\n",
    "if not os.path.isfile(csv_path):\n",
    "    # dataFrame 생성\n",
    "    df = pd.DataFrame(csvList)\n",
    "    # 0번 col을 sorting -> 0번 row 부터 차례대로 읽어도 label(16384)은 sorting 안돼있음\n",
    "    df = df.sort_values(0)\n",
    "    # csv파일로 저장, (index 및 header 제외)\n",
    "    df.to_csv(csv_path, sep=',', header=False, index=False)\n",
    "else:\n",
    "    print('file already exists...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./resources/csv/gender/train_gender_img_label_data.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "train_img_data = data[:, 0:-1]\n",
    "train_label_data = data[:, [-1]]\n",
    "\n",
    "# min-max scaling\n",
    "train_img_data = (train_img_data - train_img_data.min()) / (train_img_data.max() - train_img_data.min())\n",
    "# contrast normalization\n",
    "train_img_data = (train_img_data - train_img_data.mean()) / train_img_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = './resources/images/gender/testSet/'\n",
    "Labels = os.listdir(TEST_DIR)\n",
    "csvList = []\n",
    "\n",
    "for dir in Labels:\n",
    "    img_paths = glob.glob(TEST_DIR + dir + '/*')\n",
    "    imgs = [cv2.imread(file, 0) for file in img_paths]\n",
    "    \n",
    "    for img in imgs:\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img_flat = img.flatten()\n",
    "        csvList.append(np.append(img_flat, int(dir.split('_')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists...\n"
     ]
    }
   ],
   "source": [
    "csv_path = './resources/csv/gender/test_gender_img_label_data.csv'\n",
    "\n",
    "if not os.path.isfile(csv_path):\n",
    "    # dataFrame 생성\n",
    "    df = pd.DataFrame(csvList)\n",
    "    # 0번 col을 sorting -> 0번 row 부터 차례대로 읽어도 label(16384)은 sorting 안돼있음\n",
    "    df = df.sort_values(0)\n",
    "    # csv파일로 저장, (index 및 header 제외)\n",
    "    df.to_csv(csv_path, sep=',', header=False, index=False)\n",
    "else:\n",
    "    print('file already exists...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./resources/csv/gender/test_gender_img_label_data.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "test_img_data = data[:, 0:-1]\n",
    "test_label_data = data[:, [-1]]\n",
    "\n",
    "# min-max scaling\n",
    "test_img_data = (test_img_data - test_img_data.min()) / (test_img_data.max() - test_img_data.min())\n",
    "# contrast normalization\n",
    "test_img_data = (test_img_data - test_img_data.mean()) / test_img_data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 80 * 80])\n",
    "X_img = tf.reshape(X, [-1, 80, 80, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "with tf.name_scope('L1') as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "    L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L1 = tf.nn.relu6(L1)\n",
    "    L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    " \n",
    "with tf.name_scope('L2') as scope:\n",
    "    # one by one convolution for dimension reduction\n",
    "    _W2 = tf.Variable(tf.random_normal([1, 1, 32, 5]))\n",
    "    _L2 = tf.nn.conv2d(L1, _W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([3, 3, 5, 64], stddev=0.01))\n",
    "    L2 = tf.nn.conv2d(_L2, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L2 = tf.nn.relu6(L2)\n",
    "    L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "with tf.name_scope('L3') as scope:\n",
    "    # one by one convolution for dimension reduction\n",
    "    _W3 = tf.Variable(tf.random_normal([1, 1, 64, 5]))\n",
    "    _L3 = tf.nn.conv2d(L2, _W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    W3 = tf.Variable(tf.random_normal([3, 3, 5, 128], stddev=0.01))\n",
    "    L3 = tf.nn.conv2d(_L3, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L3 = tf.nn.relu6(L3)\n",
    "    L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('L4') as scope:\n",
    "    # one by one convolution for dimension reduction\n",
    "    _W4 = tf.Variable(tf.random_normal([1, 1, 128, 5]))\n",
    "    _L4 = tf.nn.conv2d(L3, _W4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    W4 = tf.Variable(tf.random_normal([3, 3, 5, 256], stddev=0.01))\n",
    "    L4 = tf.nn.conv2d(_L4, W4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L4 = tf.nn.relu6(L4)\n",
    "    L4 = tf.nn.max_pool(L4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('L5_FC') as scope:\n",
    "    L4_flat = tf.reshape(L4, [-1, 5 * 5 * 256])\n",
    "    W5 = tf.get_variable(\"W5\", shape=[5 * 5 * 256, 5 * 5 * 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([5 * 5 * 128]))\n",
    "    L5 = tf.nn.relu6(tf.matmul(L4_flat, W5) + b5)\n",
    "    L5 = tf.nn.dropout(L5, keep_prob=keep_prob)\n",
    "\n",
    "with tf.name_scope('L6_FC') as scope:\n",
    "    W6 = tf.get_variable(\"W6\", shape=[5 * 5 * 128, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b6 = tf.Variable(tf.random_normal([1]))\n",
    "    logits = tf.matmul(L5, W6) + b6\n",
    "    hypothesis = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-08).minimize(cost)\n",
    "prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 1 / 120   cost:  0.690454098   train_Accuracy: 65.625 %   test_Accracy: 68.000 %\n",
      "Epoch: 2 / 120   cost:  0.862701305   train_Accuracy: 56.146 %   test_Accracy: 50.000 %\n",
      "Epoch: 3 / 120   cost:  0.815348876   train_Accuracy: 58.021 %   test_Accracy: 68.000 %\n",
      "Epoch: 4 / 120   cost:  0.589025825   train_Accuracy: 72.083 %   test_Accracy: 83.000 %\n",
      "Epoch: 5 / 120   cost:  0.584394175   train_Accuracy: 73.021 %   test_Accracy: 64.000 %\n",
      "Epoch: 6 / 120   cost:  0.725727340   train_Accuracy: 64.896 %   test_Accracy: 64.000 %\n",
      "Epoch: 7 / 120   cost:  0.635881128   train_Accuracy: 70.104 %   test_Accracy: 67.000 %\n",
      "Epoch: 8 / 120   cost:  0.694575413   train_Accuracy: 61.979 %   test_Accracy: 65.000 %\n",
      "Epoch: 9 / 120   cost:  0.720481724   train_Accuracy: 68.333 %   test_Accracy: 70.000 %\n",
      "Epoch: 10 / 120   cost:  0.655365318   train_Accuracy: 69.896 %   test_Accracy: 69.000 %\n",
      "Epoch: 11 / 120   cost:  0.530700558   train_Accuracy: 77.708 %   test_Accracy: 67.000 %\n",
      "Epoch: 12 / 120   cost:  0.601264701   train_Accuracy: 70.833 %   test_Accracy: 75.000 %\n",
      "Epoch: 13 / 120   cost:  0.530048706   train_Accuracy: 77.292 %   test_Accracy: 72.000 %\n",
      "Epoch: 14 / 120   cost:  0.613912133   train_Accuracy: 71.354 %   test_Accracy: 84.000 %\n",
      "Epoch: 15 / 120   cost:  0.402432201   train_Accuracy: 83.854 %   test_Accracy: 83.000 %\n",
      "Epoch: 16 / 120   cost:  0.400380051   train_Accuracy: 83.542 %   test_Accracy: 79.000 %\n",
      "Epoch: 17 / 120   cost:  0.366654939   train_Accuracy: 85.208 %   test_Accracy: 83.000 %\n",
      "Epoch: 18 / 120   cost:  0.430035337   train_Accuracy: 82.083 %   test_Accracy: 94.000 %\n",
      "Epoch: 19 / 120   cost:  0.268779510   train_Accuracy: 89.687 %   test_Accracy: 93.000 %\n",
      "Epoch: 20 / 120   cost:  0.201915491   train_Accuracy: 92.396 %   test_Accracy: 92.000 %\n",
      "Epoch: 21 / 120   cost:  0.163375087   train_Accuracy: 93.229 %   test_Accracy: 92.000 %\n",
      "Epoch: 22 / 120   cost:  0.136178448   train_Accuracy: 94.583 %   test_Accracy: 92.000 %\n",
      "Epoch: 23 / 120   cost:  0.114817561   train_Accuracy: 95.729 %   test_Accracy: 93.000 %\n",
      "Epoch: 24 / 120   cost:  0.076082771   train_Accuracy: 97.708 %   test_Accracy: 95.000 %\n",
      "Epoch: 25 / 120   cost:  0.074995298   train_Accuracy: 97.500 %   test_Accracy: 95.000 %\n",
      "Epoch: 26 / 120   cost:  0.045551819   train_Accuracy: 98.438 %   test_Accracy: 94.000 %\n",
      "Epoch: 27 / 120   cost:  0.046608564   train_Accuracy: 98.542 %   test_Accracy: 97.000 %\n",
      "Epoch: 28 / 120   cost:  0.039811648   train_Accuracy: 98.854 %   test_Accracy: 95.000 %\n",
      "Epoch: 29 / 120   cost:  0.037453978   train_Accuracy: 98.854 %   test_Accracy: 98.000 %\n",
      "Epoch: 30 / 120   cost:  0.035203467   train_Accuracy: 98.854 %   test_Accracy: 94.000 %\n",
      "Epoch: 31 / 120   cost:  0.062292441   train_Accuracy: 97.708 %   test_Accracy: 89.000 %\n",
      "Epoch: 32 / 120   cost:  0.106319254   train_Accuracy: 95.417 %   test_Accracy: 88.000 %\n",
      "Epoch: 33 / 120   cost:  0.118238355   train_Accuracy: 94.479 %   test_Accracy: 89.000 %\n",
      "Epoch: 34 / 120   cost:  0.161038237   train_Accuracy: 93.333 %   test_Accracy: 85.000 %\n",
      "Epoch: 35 / 120   cost:  0.167038483   train_Accuracy: 94.062 %   test_Accracy: 93.000 %\n",
      "Epoch: 36 / 120   cost:  0.100450600   train_Accuracy: 96.042 %   test_Accracy: 94.000 %\n",
      "Epoch: 37 / 120   cost:  0.067581874   train_Accuracy: 97.708 %   test_Accracy: 91.000 %\n",
      "Epoch: 38 / 120   cost:  0.037275162   train_Accuracy: 98.750 %   test_Accracy: 96.000 %\n",
      "Epoch: 39 / 120   cost:  0.010995130   train_Accuracy: 99.896 %   test_Accracy: 96.000 %\n",
      "Epoch: 40 / 120   cost:  0.007895472   train_Accuracy: 99.792 %   test_Accracy: 97.000 %\n",
      "Epoch: 41 / 120   cost:  0.004964292   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 42 / 120   cost:  0.001957758   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 43 / 120   cost:  0.002109191   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 44 / 120   cost:  0.001259916   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 45 / 120   cost:  0.001104437   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 46 / 120   cost:  0.000768699   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 47 / 120   cost:  0.000810197   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 48 / 120   cost:  0.000833574   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 49 / 120   cost:  0.000756957   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 50 / 120   cost:  0.000604631   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 51 / 120   cost:  0.000729944   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 52 / 120   cost:  0.000494124   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 53 / 120   cost:  0.000424344   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 54 / 120   cost:  0.000367082   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 55 / 120   cost:  0.000343216   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 56 / 120   cost:  0.000343458   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 57 / 120   cost:  0.000283841   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 58 / 120   cost:  0.000256153   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 59 / 120   cost:  0.000268328   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 60 / 120   cost:  0.000250212   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 61 / 120   cost:  0.000248326   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 62 / 120   cost:  0.000221611   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 63 / 120   cost:  0.000230510   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 64 / 120   cost:  0.000190628   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 65 / 120   cost:  0.000177992   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 66 / 120   cost:  0.000183405   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 67 / 120   cost:  0.000155385   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 68 / 120   cost:  0.000153478   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 69 / 120   cost:  0.000150339   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 70 / 120   cost:  0.000107841   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 71 / 120   cost:  0.000119046   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 72 / 120   cost:  0.000101932   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 73 / 120   cost:  0.000101334   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 74 / 120   cost:  0.000101907   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 75 / 120   cost:  0.000089860   train_Accuracy: 100.000 %   test_Accracy: 96.000 %\n",
      "Epoch: 76 / 120   cost:  0.000079580   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 77 / 120   cost:  0.000079313   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 78 / 120   cost:  0.000073305   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 79 / 120   cost:  0.000077874   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 80 / 120   cost:  0.000068917   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 81 / 120   cost:  0.000061394   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 82 / 120   cost:  0.000056402   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 83 / 120   cost:  0.000062632   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 84 / 120   cost:  0.000060439   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 85 / 120   cost:  0.000044549   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 86 / 120   cost:  0.000039459   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 87 / 120   cost:  0.000043748   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 88 / 120   cost:  0.000041826   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 89 / 120   cost:  0.000034458   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 90 / 120   cost:  0.000041246   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 91 / 120   cost:  0.000049045   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 92 / 120   cost:  0.000038816   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 / 120   cost:  0.000034927   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 94 / 120   cost:  0.000035678   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 95 / 120   cost:  0.000033445   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 96 / 120   cost:  0.000024408   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 97 / 120   cost:  0.000031649   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 98 / 120   cost:  0.000025340   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 99 / 120   cost:  0.000027395   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 100 / 120   cost:  0.000025816   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 101 / 120   cost:  0.000020854   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 102 / 120   cost:  0.000026736   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 103 / 120   cost:  0.000026734   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 104 / 120   cost:  0.000034335   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 105 / 120   cost:  0.000024330   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 106 / 120   cost:  0.000020899   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 107 / 120   cost:  0.000017072   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 108 / 120   cost:  0.000025240   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 109 / 120   cost:  0.000024884   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 110 / 120   cost:  0.000013915   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 111 / 120   cost:  0.000015433   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 112 / 120   cost:  0.000016211   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 113 / 120   cost:  0.000014288   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 114 / 120   cost:  0.000016865   train_Accuracy: 100.000 %   test_Accracy: 98.000 %\n",
      "Epoch: 115 / 120   cost:  0.000013255   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 116 / 120   cost:  0.000014985   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 117 / 120   cost:  0.000009408   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 118 / 120   cost:  0.000013766   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 119 / 120   cost:  0.000014361   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Epoch: 120 / 120   cost:  0.000013321   train_Accuracy: 100.000 %   test_Accracy: 97.000 %\n",
      "Learning Finished!\n",
      "Elapsed Time:  555.9236981868744\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 120\n",
    "batch_size = 64\n",
    "save_model_path = './resources/models/'\n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # for elapsed time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter('./resources/tensorBoard/gender/train/', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./resources/tensorBoard/gender/test/')\n",
    "    \n",
    "    # train my model\n",
    "    print('Learning started. It takes sometime.')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        avg_acc = 0\n",
    "        total_batch = int(len(train_img_data) / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            start = ((i+1) * batch_size) - batch_size\n",
    "            end = ((i+1) * batch_size)\n",
    "\n",
    "            batch_xs = train_img_data[start:end]\n",
    "            batch_ys = train_label_data[start:end]\n",
    "            feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.5}\n",
    "\n",
    "            _, c, train_acc, train_s = sess.run([train, cost, accuracy, summary], feed_dict=feed_dict)\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += train_acc / total_batch\n",
    "\n",
    "        test_acc, test_s = sess.run([accuracy, summary], feed_dict={X: test_img_data, Y: test_label_data, keep_prob: 1})\n",
    "        \n",
    "        print('Epoch:', '%d / %d' % (epoch + 1, training_epochs), '  cost: ', '{:.9f}'.format(avg_cost), '  train_Accuracy: %.3f' % (avg_acc * 100), '%', '  test_Accracy: %.3f' % (test_acc * 100), '%')\n",
    "        \n",
    "        train_writer.add_summary(train_s, global_step=epoch)\n",
    "        test_writer.add_summary(test_s, global_step=epoch)\n",
    "\n",
    "    print('Learning Finished!')\n",
    "    print('Elapsed Time: ', time.time() - start_time)\n",
    "    \n",
    "    saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./resources/models/\n",
      "Accuracy:  97.00 %\n",
      "===============================\n",
      "[[9.9914503e-01 1.0000000e+00]\n",
      " [9.9999702e-01 1.0000000e+00]\n",
      " [4.9971472e-02 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [3.5288518e-03 0.0000000e+00]\n",
      " [9.7799748e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [9.9999988e-01 1.0000000e+00]\n",
      " [9.9664247e-01 1.0000000e+00]\n",
      " [9.9999988e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [9.9999928e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [6.9201875e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [3.9332044e-13 0.0000000e+00]\n",
      " [4.4465014e-05 0.0000000e+00]\n",
      " [9.9999642e-01 0.0000000e+00]\n",
      " [7.9183560e-03 1.0000000e+00]\n",
      " [9.9999976e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [9.9999964e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [9.9891484e-01 1.0000000e+00]\n",
      " [9.9999857e-01 1.0000000e+00]\n",
      " [1.1254170e-13 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [8.5980195e-10 0.0000000e+00]\n",
      " [4.6878296e-11 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [9.3229913e-10 0.0000000e+00]\n",
      " [9.3229913e-10 0.0000000e+00]\n",
      " [3.8618214e-07 0.0000000e+00]\n",
      " [1.0782518e-04 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [5.1884834e-02 0.0000000e+00]\n",
      " [4.9758822e-01 1.0000000e+00]\n",
      " [9.8933113e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [3.8536516e-09 0.0000000e+00]\n",
      " [1.8305088e-10 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.2152138e-04 0.0000000e+00]\n",
      " [9.9995184e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [8.7267829e-08 0.0000000e+00]\n",
      " [4.0334611e-11 0.0000000e+00]\n",
      " [2.3781749e-05 0.0000000e+00]\n",
      " [9.9999785e-01 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [2.1633656e-12 0.0000000e+00]\n",
      " [3.6300295e-07 0.0000000e+00]\n",
      " [1.0565052e-14 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [2.3791792e-13 0.0000000e+00]\n",
      " [5.1561012e-11 0.0000000e+00]\n",
      " [4.6461159e-07 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [2.8888804e-03 0.0000000e+00]\n",
      " [8.5947764e-13 0.0000000e+00]\n",
      " [3.8750926e-03 0.0000000e+00]\n",
      " [6.8953689e-03 0.0000000e+00]\n",
      " [4.0522126e-12 0.0000000e+00]\n",
      " [9.9999952e-01 1.0000000e+00]\n",
      " [4.5640917e-14 0.0000000e+00]\n",
      " [1.2118924e-06 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [4.0019200e-11 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [5.3917262e-07 0.0000000e+00]\n",
      " [3.3565979e-02 0.0000000e+00]\n",
      " [4.0264989e-10 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.5046653e-06 0.0000000e+00]\n",
      " [2.3857653e-03 0.0000000e+00]\n",
      " [2.4000859e-13 0.0000000e+00]\n",
      " [1.5169444e-12 0.0000000e+00]\n",
      " [4.1773930e-07 0.0000000e+00]\n",
      " [2.9107002e-07 0.0000000e+00]\n",
      " [3.5675761e-12 0.0000000e+00]\n",
      " [1.0831039e-10 0.0000000e+00]\n",
      " [9.9999988e-01 1.0000000e+00]\n",
      " [4.7454163e-10 0.0000000e+00]\n",
      " [2.1127075e-08 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [3.0069696e-08 0.0000000e+00]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [3.1747217e-07 0.0000000e+00]\n",
      " [1.4275131e-08 0.0000000e+00]\n",
      " [2.1305448e-16 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './resources/models/'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_model_path)\n",
    "    \n",
    "    a, h, y = sess.run([accuracy, hypothesis, Y], feed_dict={X: test_img_data, Y: test_label_data, keep_prob: 1})\n",
    "    \n",
    "    print('Accuracy: ', '{:.2f}'.format(a * 100), '%')\n",
    "    print('===============================')\n",
    "    print(sess.run(tf.concat([h, y], 1)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
